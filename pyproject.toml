[build-system]
requires = ["setuptools>=65.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "kttc"
version = "0.1.0"
description = "Knowledge Translation Transmutation Core - Transforming translations into gold-standard quality"
readme = "README.md"
requires-python = ">=3.11"
license = {text = "Apache-2.0"}
authors = [
    {name = "KTTC AI", email = "dev@kt.tc"}
]
keywords = ["translation", "qa", "quality-assurance", "llm", "agents", "automation"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: Apache Software License",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Quality Assurance",
    "Topic :: Software Development :: Testing",
]

dependencies = [
    # Core CLI dependencies (lightweight, ~50MB)
    "typer>=0.20.0",
    "rich>=14.2.0",
    "pydantic>=2.12.4",
    "pydantic-settings>=2.11.0",
    "aiohttp>=3.13.2",
    "openai>=2.7.1",
    "anthropic>=0.72.0",
    "aiosqlite>=0.21.0",
    "orjson>=3.11.4",
    "fastapi>=0.121.1",
    "uvicorn>=0.38.0",
    "websockets>=15.0.1",
    "sacrebleu>=2.5.1",
    "mtdata>=0.4.3",
]

[project.optional-dependencies]
# Neural metrics - heavy dependencies (~2GB with PyTorch)
# Required for production use: pip install kttc[neural]
neural = [
    "unbabel-comet>=2.2.7",
    "sentence-transformers>=5.1.2",
    "torch>=2.0.0",  # Explicit for clarity
]

# Full installation (includes neural metrics)
# Use for local development: pip install -e ".[full]"
full = [
    "unbabel-comet>=2.2.7",
    "sentence-transformers>=5.1.2",
    "torch>=2.0.0",
]

# Development tools (excludes neural to keep CI fast)
# Use for testing: pip install -e ".[dev]"
dev = [
    "pytest>=9.0.0",
    "pytest-cov>=7.0.0",
    "pytest-xdist>=3.8.0",
    "pytest-asyncio>=0.23.0",
    "black>=25.11.0",
    "ruff>=0.14.4",
    "mypy>=1.18.2",
    "pre-commit>=4.4.0",
    "types-requests>=2.32.4",
]

[project.scripts]
kttc = "kttc.cli.main:app"

[project.urls]
Homepage = "https://github.com/kttc-ai/kttc"
Documentation = "https://github.com/kttc-ai/docs"
Repository = "https://github.com/kttc-ai/kttc"
Issues = "https://github.com/kttc-ai/kttc/issues"

# Ruff configuration
[tool.ruff]
line-length = 100
target-version = "py311"

[tool.ruff.lint]
select = ["E", "F", "I", "N", "W", "UP"]
ignore = ["E501"]

[tool.ruff.lint.per-file-ignores]
"tests/*" = ["ANN"]

# Black configuration
[tool.black]
line-length = 100
target-version = ['py311']

# Mypy configuration
[tool.mypy]
python_version = "3.11"
strict = true
warn_return_any = true
warn_unused_configs = true
files = ["src/kttc"]

# Ignore missing imports for optional dependencies without type stubs
[[tool.mypy.overrides]]
module = ["huggingface_hub", "huggingface_hub.*", "sentence_transformers", "sentence_transformers.*", "comet", "comet.*"]
ignore_missing_imports = true

# Pytest configuration
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
addopts = [
    "--strict-markers",
    "--cov=kttc",
    "--cov-report=term-missing",
    "--cov-report=html",
    "-v",
]
markers = [
    "benchmark: mark test as performance benchmark",
    "slow: mark test as slow-running",
    "metrics: mark test as requiring heavy ML metrics dependencies",
]

# Coverage configuration
[tool.coverage.run]
source = ["src/kttc"]
omit = ["*/tests/*", "*/conftest.py"]

[tool.coverage.report]
precision = 2
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise NotImplementedError",
    "if TYPE_CHECKING:",
    "@abstractmethod",
]

# Vulture configuration (dead code detection) - STRICT
[tool.vulture]
min_confidence = 60  # Lower threshold to catch more potential dead code
paths = ["src/kttc"]
exclude = ["tests/", "examples/", "__pycache__/"]
ignore_decorators = ["@app.route", "@app.on_event", "@app.get", "@app.post", "@app.websocket"]
ignore_names = ["test_*", "setUp", "tearDown", "main", "app"]
sort_by_size = true

# Bandit configuration (security scanning) - STRICT
[tool.bandit]
exclude_dirs = ["tests/", "examples/", ".venv/", "venv/", "__pycache__/"]
# Removed B101 skip - will check all issues including asserts
tests = ["B201", "B301", "B302", "B303", "B304", "B305", "B306", "B307", "B308", "B309", "B310", "B311", "B312", "B313", "B314", "B315", "B316", "B317", "B318", "B319", "B320", "B321", "B322", "B323", "B324", "B325", "B401", "B402", "B403", "B404", "B405", "B406", "B407", "B408", "B409", "B410", "B411", "B412", "B413", "B501", "B502", "B503", "B504", "B505", "B506", "B507", "B601", "B602", "B603", "B604", "B605", "B606", "B607", "B608", "B609", "B610", "B611", "B612", "B701", "B702", "B703"]
severity = "low"  # Check all severity levels
confidence = "low"  # Check all confidence levels

# Radon configuration (complexity metrics) - STRICT
[tool.radon]
cc_min = "A"  # Strict: Only allow grade A complexity
mi_min = 20  # Minimum maintainability index (0-100 scale)
show_complexity = true
average = true
