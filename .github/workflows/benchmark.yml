name: WMT Benchmark

on:
  schedule:
    # Run weekly on Sunday at midnight UTC
    - cron: '0 0 * * 0'

  workflow_dispatch:
    inputs:
      dataset:
        description: 'Benchmark dataset'
        required: true
        default: 'flores-200'
        type: choice
        options:
          - flores-200
          - wmt23
          - wmt22
      language-pair:
        description: 'Language pair (e.g., eng_Latn-spa_Latn or en-es)'
        required: true
        default: 'eng_Latn-spa_Latn'
      sample-size:
        description: 'Number of samples to evaluate'
        required: false
        default: '100'
      compare-baseline:
        description: 'Compare with baseline systems'
        required: false
        default: 'false'
        type: boolean

jobs:
  benchmark:
    name: Run WMT Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run WMT Benchmark
        env:
          KTTC_OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          KTTC_ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          KTTC_GIGACHAT_CLIENT_ID: ${{ secrets.GIGACHAT_CLIENT_ID }}
          KTTC_GIGACHAT_CLIENT_SECRET: ${{ secrets.GIGACHAT_CLIENT_SECRET }}
        run: |
          python3.11 -c "
          import asyncio
          from tests.benchmarks.wmt_benchmark import WMTBenchmark
          from kttc.agents.orchestrator import AgentOrchestrator
          from kttc.llm import OpenAIProvider

          async def main():
              # Initialize
              llm = OpenAIProvider(model='gpt-4')
              orchestrator = AgentOrchestrator(llm)

              # Run benchmark
              benchmark = WMTBenchmark('benchmark_results')
              result = await benchmark.run_benchmark(
                  orchestrator=orchestrator,
                  dataset_name='${{ inputs.dataset || 'flores-200' }}',
                  language_pair='${{ inputs.language-pair || 'eng_Latn-spa_Latn' }}',
                  sample_size=int('${{ inputs.sample-size || '100' }}'),
              )

              # Export reports
              benchmark.export_report('benchmark_report.md', format='markdown')
              benchmark.export_report('benchmark_report.json', format='json')

              print('‚úÖ Benchmark completed!')
              print(f'MQM Score: {result.avg_mqm_score:.2f}¬±{result.std_mqm_score:.2f}')
              if result.avg_comet_score:
                  print(f'COMET Score: {result.avg_comet_score:.3f}')

          asyncio.run(main())
          "

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}
          path: |
            benchmark_results/
            benchmark_report.md
            benchmark_report.json
          retention-days: 90

      - name: Comment on PR with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');

            try {
              const report = JSON.parse(fs.readFileSync('benchmark_report.json', 'utf8'));
              const latestResult = report.results[report.results.length - 1];

              let comment = '## üìä WMT Benchmark Results\n\n';
              comment += `**Dataset:** ${latestResult.dataset_name}\n`;
              comment += `**Sample Size:** ${latestResult.sample_size}\n\n`;
              comment += `### Quality Metrics\n\n`;
              comment += `| Metric | Score | Status |\n`;
              comment += `|--------|-------|--------|\n`;
              comment += `| MQM Score | ${latestResult.avg_mqm_score.toFixed(2)} ¬± ${latestResult.std_mqm_score.toFixed(2)} | ${latestResult.avg_mqm_score >= 95 ? '‚úÖ' : '‚ö†Ô∏è'} |\n`;

              if (latestResult.avg_comet_score) {
                comment += `| COMET | ${latestResult.avg_comet_score.toFixed(3)} | ${latestResult.avg_comet_score >= 0.85 ? '‚úÖ' : '‚ö†Ô∏è'} |\n`;
              }

              if (latestResult.avg_kiwi_score) {
                comment += `| CometKiwi | ${latestResult.avg_kiwi_score.toFixed(3)} | ${latestResult.avg_kiwi_score >= 0.80 ? '‚úÖ' : '‚ö†Ô∏è'} |\n`;
              }

              comment += `\n### Error Distribution\n\n`;
              comment += `- Critical: ${latestResult.error_counts.critical || 0}\n`;
              comment += `- Major: ${latestResult.error_counts.major || 0}\n`;
              comment += `- Minor: ${latestResult.error_counts.minor || 0}\n`;
              comment += `\n### Performance\n\n`;
              comment += `- Average processing time: ${latestResult.avg_processing_time.toFixed(2)}s per translation\n`;
              comment += `\nüìÑ [View full report](${context.payload.repository.html_url}/actions/runs/${context.runId})\n`;

              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Could not post benchmark results:', error.message);
            }

      - name: Create benchmark summary
        if: always()
        run: |
          echo "# Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f benchmark_report.json ]; then
            MQM_SCORE=$(jq -r '.results[-1].avg_mqm_score' benchmark_report.json)
            SAMPLE_SIZE=$(jq -r '.results[-1].sample_size' benchmark_report.json)
            DATASET=$(jq -r '.results[-1].dataset_name' benchmark_report.json)

            echo "‚úÖ **Benchmark completed successfully**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "- **Dataset:** $DATASET" >> $GITHUB_STEP_SUMMARY
            echo "- **Samples:** $SAMPLE_SIZE" >> $GITHUB_STEP_SUMMARY
            echo "- **MQM Score:** $MQM_SCORE" >> $GITHUB_STEP_SUMMARY

            # Check if meets quality threshold
            THRESHOLD=95.0
            if (( $(echo "$MQM_SCORE >= $THRESHOLD" | bc -l) )); then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "üéâ **Quality threshold met!** ($MQM_SCORE >= $THRESHOLD)" >> $GITHUB_STEP_SUMMARY
            else
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "‚ö†Ô∏è **Below quality threshold** ($MQM_SCORE < $THRESHOLD)" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "‚ùå **Benchmark failed** - no results generated" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Store benchmark history
        if: success()
        run: |
          # Store results for trend analysis
          mkdir -p .benchmark-history
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          cp benchmark_report.json ".benchmark-history/benchmark-${TIMESTAMP}.json"

          echo "üìä Benchmark history updated"

      - name: Generate benchmark statistics
        if: inputs.compare-baseline == 'true' || inputs.compare-baseline == true
        run: |
          echo "üìä Generating benchmark statistics..."

          # Calculate statistics from benchmark results
          python3.11 -c "
          import json
          from pathlib import Path

          results_dir = Path('benchmark_results')
          if results_dir.exists():
              json_files = list(results_dir.glob('*.json'))
              print(f'Found {len(json_files)} benchmark result files')

              if json_files:
                  # Load latest result
                  latest = sorted(json_files)[-1]
                  with open(latest) as f:
                      data = json.load(f)

                  print('\nüìà Latest Benchmark Statistics:')
                  for result in data.get('results', []):
                      print(f\"Dataset: {result['dataset_name']}\")
                      print(f\"MQM Score: {result['avg_mqm_score']:.2f} ¬± {result['std_mqm_score']:.2f}\")
                      print(f\"Sample Size: {result['sample_size']}\")
                      if result.get('avg_comet_score'):
                          print(f\"COMET Score: {result['avg_comet_score']:.3f}\")
                      print()
          else:
              print('No benchmark results found')
          "

          echo "‚úÖ Statistics generation completed"
