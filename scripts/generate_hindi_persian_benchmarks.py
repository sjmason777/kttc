"""Generate benchmark data for Hindi and Persian languages.

This script creates complete benchmark datasets for Hindi and Persian:
- flores200 dev/devtest (reference translations)
- quality (clean LLM translations)
- synthetic_bad (with intentional errors)
- critical_bad (critical translation errors)
"""

import json
from pathlib import Path

# Base directory for benchmark data
BENCHMARK_DIR = Path(__file__).parent.parent / "tests" / "benchmarks" / "data"

# Sample sentences for benchmarking
ENGLISH_SENTENCES = [
    "Artificial intelligence is transforming the world.",
    "Machine translation quality has improved significantly.",
    "Natural language processing enables human-computer interaction.",
    "Deep learning models process vast amounts of data.",
    "Neural networks mimic human brain structure.",
    "Cloud computing provides scalable infrastructure.",
    "Cybersecurity protects sensitive information.",
    "Blockchain technology ensures data integrity.",
    "Quantum computing promises exponential speedup.",
    "Internet of Things connects everyday devices.",
]

# Hindi translations (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)
HINDI_TRANSLATIONS = [
    "‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§ï‡•ã ‡§¨‡§¶‡§≤ ‡§∞‡§π‡•Ä ‡§π‡•à‡•§",
    "‡§Æ‡§∂‡•Ä‡§®‡•Ä ‡§Ö‡§®‡•Å‡§µ‡§æ‡§¶ ‡§ï‡•Ä ‡§ó‡•Å‡§£‡§µ‡§§‡•ç‡§§‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡§æ‡§´‡•Ä ‡§∏‡•Å‡§ß‡§æ‡§∞ ‡§π‡•Å‡§Ü ‡§π‡•à‡•§",
    "‡§™‡•ç‡§∞‡§æ‡§ï‡•É‡§§‡§ø‡§ï ‡§≠‡§æ‡§∑‡§æ ‡§™‡•ç‡§∞‡§∏‡§Ç‡§∏‡•ç‡§ï‡§∞‡§£ ‡§Æ‡§æ‡§®‡§µ-‡§ï‡§Ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡§∞ ‡§∏‡§Ç‡§µ‡§æ‡§¶ ‡§ï‡•ã ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à‡•§",
    "‡§°‡•Ä‡§™ ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§Æ‡•â‡§°‡§≤ ‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ ‡§Æ‡•á‡§Ç ‡§°‡•á‡§ü‡§æ ‡§ï‡•ã ‡§∏‡§Ç‡§∏‡§æ‡§ß‡§ø‡§§ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§",
    "‡§§‡§Ç‡§§‡•ç‡§∞‡§ø‡§ï‡§æ ‡§®‡•á‡§ü‡§µ‡§∞‡•ç‡§ï ‡§Æ‡§æ‡§®‡§µ ‡§Æ‡§∏‡•ç‡§§‡§ø‡§∑‡•ç‡§ï ‡§ï‡•Ä ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ ‡§ï‡•Ä ‡§®‡§ï‡§≤ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§",
    "‡§ï‡•ç‡§≤‡§æ‡§â‡§° ‡§ï‡§Ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡§ø‡§Ç‡§ó ‡§∏‡•ç‡§ï‡•á‡§≤‡•á‡§¨‡§≤ ‡§¨‡•Å‡§®‡§ø‡§Ø‡§æ‡§¶‡•Ä ‡§¢‡§æ‡§Ç‡§ö‡§æ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§",
    "‡§∏‡§æ‡§á‡§¨‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§∏‡§Ç‡§µ‡•á‡§¶‡§®‡§∂‡•Ä‡§≤ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡•Ä ‡§∞‡§ï‡•ç‡§∑‡§æ ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à‡•§",
    "‡§¨‡•ç‡§≤‡•â‡§ï‡§ö‡•á‡§® ‡§§‡§ï‡§®‡•Ä‡§ï ‡§°‡•á‡§ü‡§æ ‡§ï‡•Ä ‡§Ö‡§ñ‡§Ç‡§°‡§§‡§æ ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à‡•§",
    "‡§ï‡•ç‡§µ‡§æ‡§Ç‡§ü‡§Æ ‡§ï‡§Ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡§ø‡§Ç‡§ó ‡§ò‡§æ‡§§‡§æ‡§Ç‡§ï‡•Ä‡§Ø ‡§ó‡§§‡§ø ‡§ï‡§æ ‡§µ‡§æ‡§¶‡§æ ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à‡•§",
    "‡§á‡§Ç‡§ü‡§∞‡§®‡•á‡§ü ‡§ë‡§´ ‡§•‡§ø‡§Ç‡§ó‡•ç‡§∏ ‡§∞‡•ã‡§ú‡§Æ‡§∞‡•ç‡§∞‡§æ ‡§ï‡•á ‡§â‡§™‡§ï‡§∞‡§£‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡•ã‡§°‡§º‡§§‡§æ ‡§π‡•à‡•§",
]

# Persian translations (ŸÅÿßÿ±ÿ≥€å)
PERSIAN_TRANSLATIONS = [
    "ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å ÿØÿ± ÿ≠ÿßŸÑ ÿ™ÿ∫€å€åÿ± ÿ¨ŸáÿßŸÜ ÿßÿ≥ÿ™.",
    "⁄©€åŸÅ€åÿ™ ÿ™ÿ±ÿ¨ŸÖŸá ŸÖÿßÿ¥€åŸÜ€å ÿ®Ÿá ÿ∑Ÿàÿ± ŸÇÿßÿ®ŸÑ ÿ™Ÿàÿ¨Ÿá€å ÿ®Ÿáÿ®ŸàÿØ €åÿßŸÅÿ™Ÿá ÿßÿ≥ÿ™.",
    "Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ≤ÿ®ÿßŸÜ ÿ∑ÿ®€åÿπ€å ÿ™ÿπÿßŸÖŸÑ ÿßŸÜÿ≥ÿßŸÜ Ÿà ⁄©ÿßŸÖŸæ€åŸàÿ™ÿ± ÿ±ÿß ÿßŸÖ⁄©ÿßŸÜ‚ÄåŸæÿ∞€åÿ± ŸÖ€å‚Äåÿ≥ÿßÿ≤ÿØ.",
    "ŸÖÿØŸÑ‚ÄåŸáÿß€å €åÿßÿØ⁄Ø€åÿ±€å ÿπŸÖ€åŸÇ ŸÖŸÇÿßÿØ€åÿ± ÿπÿ∏€åŸÖ€å ÿßÿ≤ ÿØÿßÿØŸá‚ÄåŸáÿß ÿ±ÿß Ÿæÿ±ÿØÿßÿ≤ÿ¥ ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ.",
    "ÿ¥ÿ®⁄©Ÿá‚ÄåŸáÿß€å ÿπÿµÿ®€å ÿ≥ÿßÿÆÿ™ÿßÿ± ŸÖÿ∫ÿ≤ ÿßŸÜÿ≥ÿßŸÜ ÿ±ÿß ÿ™ŸÇŸÑ€åÿØ ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ.",
    "ÿ±ÿß€åÿßŸÜÿ¥ ÿßÿ®ÿ±€å ÿ≤€åÿ±ÿ≥ÿßÿÆÿ™ ŸÖŸÇ€åÿßÿ≥‚ÄåŸæÿ∞€åÿ± ÿ±ÿß ŸÅÿ±ÿßŸáŸÖ ŸÖ€å‚Äå⁄©ŸÜÿØ.",
    "ÿßŸÖŸÜ€åÿ™ ÿ≥ÿß€åÿ®ÿ±€å ÿßÿ≤ ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿ≠ÿ≥ÿßÿ≥ ŸÖÿ≠ÿßŸÅÿ∏ÿ™ ŸÖ€å‚Äå⁄©ŸÜÿØ.",
    "ŸÅŸÜÿßŸàÿ±€å ÿ®ŸÑÿß⁄©⁄Ü€åŸÜ €å⁄©Ÿæÿßÿ±⁄Ü⁄Ø€å ÿØÿßÿØŸá‚ÄåŸáÿß ÿ±ÿß ÿ™ÿ∂ŸÖ€åŸÜ ŸÖ€å‚Äå⁄©ŸÜÿØ.",
    "ŸÖÿ≠ÿßÿ≥ÿ®ÿßÿ™ ⁄©ŸàÿßŸÜÿ™ŸàŸÖ€å ÿ≥ÿ±ÿπÿ™ ŸÜŸÖÿß€å€å ÿ±ÿß ŸàÿπÿØŸá ŸÖ€å‚ÄåÿØŸáÿØ.",
    "ÿß€åŸÜÿ™ÿ±ŸÜÿ™ ÿßÿ¥€åÿß ÿØÿ≥ÿ™⁄ØÿßŸá‚ÄåŸáÿß€å ÿ±Ÿàÿ≤ŸÖÿ±Ÿá ÿ±ÿß ÿ®Ÿá ŸáŸÖ ŸÖÿ™ÿµŸÑ ŸÖ€å‚Äå⁄©ŸÜÿØ.",
]


def generate_flores200_data(
    source_lang: str, target_lang: str, translations: list[str]
) -> list[dict]:
    """Generate FLORES200-style reference data."""
    data = []
    for i, (source, translation) in enumerate(zip(ENGLISH_SENTENCES, translations)):
        data.append(
            {
                "id": f"fallback_{source_lang}-{target_lang}_{i}",
                "source": source if source_lang == "en" else translation,
                "translation": translation if source_lang == "en" else source,
                "source_lang": source_lang,
                "target_lang": target_lang,
                "domain": "general",
                "dataset": "fallback",
            }
        )
    return data


def generate_quality_data(
    source_lang: str, target_lang: str, translations: list[str]
) -> list[dict]:
    """Generate quality LLM translation data."""
    data = []
    for i, (source, translation) in enumerate(zip(ENGLISH_SENTENCES, translations)):
        data.append(
            {
                "id": f"quality_{source_lang}-{target_lang}_{i}",
                "source": source if source_lang == "en" else translation,
                "translation": translation if source_lang == "en" else source,
                "source_lang": source_lang,
                "target_lang": target_lang,
                "domain": "diverse",
                "dataset": "quality_llm",
            }
        )
    return data


def introduce_error(text: str, error_type: str, lang: str) -> str:
    """Introduce synthetic errors in translation."""
    if error_type == "addition":
        # Add extra word
        if lang == "hi":
            return text.replace("‡•§", " ‡§Ü‡§ú‡•§")  # Add "today"
        if lang == "fa":
            return text.replace(".", " ÿßŸÖÿ±Ÿàÿ≤.")  # Add "today"
        return text.replace(".", " today.")
    if error_type == "omission":
        # Remove a word
        words = text.split()
        if len(words) > 3:
            return " ".join(words[:-2] + [words[-1]])
    elif error_type == "mistranslation":
        # Wrong word
        if lang == "hi":
            return text.replace("‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ", "‡§ñ‡•Å‡§´‡§ø‡§Ø‡§æ")  # AI -> intelligence (wrong context)
        if lang == "fa":
            return text.replace("ŸáŸàÿ¥", "ÿßÿ∑ŸÑÿßÿπÿßÿ™")  # AI -> information (wrong)
        return text.replace("artificial", "natural")
    return text


def generate_synthetic_bad_data(
    source_lang: str, target_lang: str, translations: list[str]
) -> list[dict]:
    """Generate synthetic bad translation data with intentional errors."""
    data = []
    error_types = ["addition", "omission", "mistranslation"]

    for i, (source, translation) in enumerate(zip(ENGLISH_SENTENCES, translations)):
        # Good translation
        data.append(
            {
                "id": f"fallback_{source_lang}-{target_lang}_{i*2}",
                "source": source if source_lang == "en" else translation,
                "translation": translation if source_lang == "en" else source,
                "source_lang": source_lang,
                "target_lang": target_lang,
                "domain": "general",
                "dataset": "fallback",
                "quality": "good",
                "expected_mqm_range": [95, 100],
            }
        )

        # Bad translation (if we have errors defined)
        if i < len(error_types):
            error_type = error_types[i % len(error_types)]
            bad_translation = introduce_error(
                translation if source_lang == "en" else source,
                error_type,
                target_lang if source_lang == "en" else source_lang,
            )
            data.append(
                {
                    "id": f"fallback_{source_lang}-{target_lang}_{i*2+1}",
                    "source": source if source_lang == "en" else translation,
                    "translation": bad_translation,
                    "source_lang": source_lang,
                    "target_lang": target_lang,
                    "domain": "general",
                    "dataset": "fallback",
                    "quality": "bad",
                    "error_type": error_type,
                    "error_severity": "minor",
                    "expected_mqm_range": [40, 75],
                }
            )

    return data


def generate_critical_bad_data(
    source_lang: str, target_lang: str, translations: list[str]
) -> list[dict]:
    """Generate critical bad translation data."""
    data = []

    for i, (source, translation) in enumerate(
        zip(ENGLISH_SENTENCES[:5], translations[:5])
    ):  # Just 5 samples
        # Introduce critical errors
        if target_lang == "hi":
            # Complete mistranslation
            bad_translation = "‡§Ø‡§π ‡§¨‡§ø‡§≤‡•ç‡§ï‡•Å‡§≤ ‡§ó‡§≤‡§§ ‡§Ö‡§®‡•Å‡§µ‡§æ‡§¶ ‡§π‡•à ‡§ú‡•ã ‡§Æ‡•Ç‡§≤ ‡§Ö‡§∞‡•ç‡§• ‡§ï‡•ã ‡§¨‡§¶‡§≤ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§"
        elif target_lang == "fa":
            bad_translation = "ÿß€åŸÜ ÿ™ÿ±ÿ¨ŸÖŸá ⁄©ÿßŸÖŸÑÿßŸã ŸÜÿßÿØÿ±ÿ≥ÿ™ ÿßÿ≥ÿ™ ⁄©Ÿá ŸÖÿπŸÜ€å ÿßÿµŸÑ€å ÿ±ÿß ÿ™ÿ∫€å€åÿ± ŸÖ€å‚ÄåÿØŸáÿØ."
        else:
            bad_translation = (
                "This is a completely wrong translation that changes the original meaning."
            )

        data.append(
            {
                "id": f"critical_{source_lang}-{target_lang}_{i}",
                "source": source if source_lang == "en" else translation,
                "translation": bad_translation,
                "source_lang": source_lang,
                "target_lang": target_lang,
                "domain": "general",
                "dataset": "critical_errors",
                "quality": "critical",
                "error_type": "complete_mistranslation",
                "error_severity": "critical",
                "expected_mqm_range": [0, 30],
            }
        )

    return data


def save_json(data: list[dict], filename: str) -> None:
    """Save data to JSON file."""
    filepath = BENCHMARK_DIR / filename
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ Created: {filename} ({len(data)} samples)")


def main():
    """Generate all benchmark data for Hindi and Persian."""
    print("=" * 60)
    print("Generating Hindi and Persian Benchmark Data")
    print("=" * 60)

    # Ensure directory exists
    BENCHMARK_DIR.mkdir(parents=True, exist_ok=True)

    # Language pairs to generate
    language_pairs = [
        ("en", "hi", HINDI_TRANSLATIONS, "Hindi"),
        ("hi", "en", HINDI_TRANSLATIONS, "Hindi"),
        ("en", "fa", PERSIAN_TRANSLATIONS, "Persian"),
        ("fa", "en", PERSIAN_TRANSLATIONS, "Persian"),
    ]

    for source_lang, target_lang, translations, lang_name in language_pairs:
        print(f"\n--- Generating {source_lang} ‚Üí {target_lang} ({lang_name}) ---")

        # FLORES200 dev and devtest
        flores_data = generate_flores200_data(source_lang, target_lang, translations)
        save_json(flores_data, f"flores200_{source_lang}_{target_lang}_dev.json")
        save_json(flores_data, f"flores200_{source_lang}_{target_lang}_devtest.json")

        # Quality data
        quality_data = generate_quality_data(source_lang, target_lang, translations)
        save_json(quality_data, f"quality_{source_lang}_{target_lang}.json")

        # Synthetic bad data
        synthetic_data = generate_synthetic_bad_data(source_lang, target_lang, translations)
        save_json(synthetic_data, f"synthetic_bad_{source_lang}_{target_lang}.json")

        # Critical bad data
        critical_data = generate_critical_bad_data(source_lang, target_lang, translations)
        save_json(critical_data, f"critical_bad_{source_lang}_{target_lang}.json")

    print("\n" + "=" * 60)
    print("‚úÖ All Hindi and Persian benchmark data generated!")
    print(f"üìÅ Location: {BENCHMARK_DIR}")
    print("=" * 60)

    # Summary
    print("\nüìä Summary:")
    print("  - Hindi: 8 files (4 pairs √ó 2 directions)")
    print("  - Persian: 8 files (4 pairs √ó 2 directions)")
    print("  - Total new files: 16")


if __name__ == "__main__":
    main()
